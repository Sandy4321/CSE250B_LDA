\documentclass[10pt]{article}

% required packages
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{multirow}

% margins
\setlength{\headwidth}{6.40in}
\pagestyle{fancy}
\addtolength{\textwidth}{1in}
\addtolength{\textheight}{1in}
\addtolength{\evensidemargin}{0.5in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\topmargin}{-0.5in}


% page headers
\fancyhead{} 
\fancyhead[LO,LE]{CSE 250B}
\fancyhead[RO,RE]{Project 3}



\title{Topic Classification using Latent Dirichlet Allocation}

\author{Adrian Guthals (aguthals@cs.ucsd.edu),\\
David Larson (dplarson@ucsd.edu),\\
\\
CSE 250B: Project \#3 \\
University of California, San Diego \\
}


\begin{document}

\maketitle


\begin{abstract}
    LDA, Gibbs sampling, topic classification of documents, datasets used, results and their meaning, conclusions
\end{abstract}



%-----------------------------------------------------------------------------
% INTRODUCTION
%-----------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Introduce topic classification of documents. Then transition into formal definitions of LDA and Gibbs Sampling.

Elkan's lecture notes \cite{CSE250B}


\subsection{Latent Dirichlet Allocation}
\label{sec:lda}

Latent Dirichlet Allocation (LDA) is

\begin{equation}
    p(\gamma | \alpha) = \frac{1}{D(\alpha)} \prod_{s=1}^{m} \gamma_{s}^{\alpha_s - 1}
\end{equation}

\begin{equation}
    D(\alpha) = \int_{\gamma} \prod_{s=1}^m \gamma_s^{\alpha_s - 1}
\end{equation}

\begin{equation}
    D(\alpha) = \frac{\prod_{s=1}^m \Gamma(\alpha_s)}{\Gamma (\sum_{s=1}^m \alpha_s )}
\end{equation}



\subsection{Gibbs Sampling}
\label{sec:gibbs}

\begin{equation}
    p(z_i = j | \bar{z}', \bar{w}) \propto \frac{q_{j w_i}' + \beta_{w_i}}{\sum_t q_{jt}' + \beta_t} \frac{n_{mj}' + \alpha_j}{\sum_k n_{mk}' + \alpha_k}
\end{equation}



%-----------------------------------------------------------------------------
% ALGORITHMS
%-----------------------------------------------------------------------------
\section{Design and Analysis of Algorithms}
\label{sec:algorithms}

Discuss how we're implementing LDA and Gibbs Sampling.



%-----------------------------------------------------------------------------
% EXPERIMENTS
%-----------------------------------------------------------------------------
\section{Design of Experiments}
\label{sec:experiments}

%
% DATASETS
%
\subsection{Datasets}
Two datasets: classic400 and KOS blog posts (from UCI Machine Learning database). KOS dataset was reduced from 3430 documents to 400 documents (vocabulary unchanged from original 6906 words).

%
% CONVERGENCE
%
\subsection{Convergence of Gibbs}
When do we decide to stop Gibbs


%
% RESULTS
%
\subsection{Results}

\begin{table}
	\centering
	\begin{tabular}{c|c|c}
		Topic 1     & Topic 2   & Topic 3 \\
		\hline
        patients    & boundary  & wing \\
        case        & layer     & mach \\
        ventricular & velocity  & supersonic \\
        system      & field     & effects \\
        research    & solution  & ratio \\
        scientific  & plate     & wings \\
        fatty       & problem   & shock \\
        nickel      & free      & numbers \\
        acids       & heat      & jet \\
        aortic      & cylinder  & lift \\
	\end{tabular}
	\caption{Ten most commonly occured words for each topic classification of the classic 400 and KOS datasets.}
	\label{tab:common_classic400}
\end{table}



%-----------------------------------------------------------------------------
% CONCLUSION
%-----------------------------------------------------------------------------
\section{Findings and Lessons Learned}
\label{sec:conclusion}

Thoughts on: LDA as a model, Gibbs Sampling as a training method, performance issues, results of the experiments


%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{sources}


\end{document}
